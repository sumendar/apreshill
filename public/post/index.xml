<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Shirin&#39;s playgRound</title>
    <link>/post/</link>
    <description>Recent content in Posts on Shirin&#39;s playgRound</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Jun 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Addendum: Text-to-Speech with the googleLanguageR package</title>
      <link>/2018/06/googlelanguager/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/googlelanguager/</guid>
      <description>After posting my short blog post about Text-to-speech with R, I got two very useful tips. One was to use the googleLanguageR package, which uses the Google Cloud Text-to-Speech API.
And indeed, it was very easy to use and the resulting audio sounded much better than what I tried before!
Here’s a short example of how to use the package for TTS:
Set up Google Cloud and authentification You first need to set up a Google Cloud Account and provide credit card information (the first year is free to use, though).</description>
    </item>
    
    <item>
      <title>Code for Workshop: Introduction to Machine Learning with R</title>
      <link>/2018/06/intro_to_ml_workshop_heidelberg/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/intro_to_ml_workshop_heidelberg/</guid>
      <description>These are the slides from my workshop: Introduction to Machine Learning with R which I gave at the University of Heidelberg, Germany on June 28th 2018. The entire code accompanying the workshop can be found below the video.
The workshop covered the basics of machine learning. With an example dataset I went through a standard machine learning workflow in R with the packages caret and h2o:
 reading in data exploratory data analysis missingness feature engineering training and test split model training with Random Forests, Gradient Boosting, Neural Nets, etc.</description>
    </item>
    
    <item>
      <title>Text-to-speech with R</title>
      <link>/2018/06/text_to_speech_r/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/text_to_speech_r/</guid>
      <description>Computers started talking to us! They do this with so called Text-to-Speech (TTS) systems. With neural nets, deep learning and lots of training data, these systems have gotten a whole lot better in recent years. In some cases, they are so good that you can’t distinguish between human and machine voice.
In one of our recent codecentric.AI videos, we compared different Text-to-Speech systems (the video is in German, though - but the text snippets and their voice recordings we show in the video are a mix of German and English).</description>
    </item>
    
    <item>
      <title>Explaining Keras image classification models with lime</title>
      <link>/2018/06/keras_fruits_lime/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/keras_fruits_lime/</guid>
      <description>Last week I published a blog post about how easy it is to train image classification models with Keras.
What I did not show in that post was how to use the model for making predictions. This, I will do here. But predictions alone are boring, so I’m adding explanations for the predictions using the lime package.
I have already written a few blog posts (here, here and here) about LIME and have given talks (here and here) about it, too.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI: Practical Deep Learning with Rachel Thomas</title>
      <link>/2018/06/twimlai138/</link>
      <pubDate>Mon, 18 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/twimlai138/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Practical Deep Learning with Rachel Thomas:
Sketchnotes from TWiMLAI talk: Practical Deep Learning with Rachel Thomas
 You can listen to the podcast here.
 In this episode, i’m joined by Rachel Thomas, founder and researcher at Fast AI. If you’re not familiar with Fast AI, the company offers a series of courses including Practical Deep Learning for Coders, Cutting Edge Deep Learning for Coders and Rachel’s Computational Linear Algebra course.</description>
    </item>
    
    <item>
      <title>It&#39;s that easy! Image classification with keras in roughly 100 lines of code.</title>
      <link>/2018/06/keras_fruits/</link>
      <pubDate>Fri, 15 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/06/keras_fruits/</guid>
      <description>I’ve been using keras and TensorFlow for a while now - and love its simplicity and straight-forward way to modeling. As part of the latest update to my Workshop about deep learning with R and keras I’ve added a new example analysis:
Building an image classifier to differentiate different types of fruits
And I was (again) suprised how fast and easy it was to build the model; it took not even half an hour and only around 100 lines of code (counting only the main code; for this post I added comments and line breaks to make it easier to read)!</description>
    </item>
    
    <item>
      <title>rOpenSci unconference 2018 &#43; introduction to TensorFlow Probability &amp; the &#39;greta&#39; package</title>
      <link>/2018/05/ropensci_unconf18/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/ropensci_unconf18/</guid>
      <description>On May 21st and 22nd, I had the honor of having been chosen to attend the rOpenSci unconference 2018 in Seattle. It was a great event and I got to meet many amazing people!
rOpenSci rOpenSci is a non-profit organisation that maintains a number of widely used R packages and is very active in promoting a community spirit around the R-world. Their core values are to have open and reproducible research, shared data and easy-to-use tools and to make all this accessible to a large number of people.</description>
    </item>
    
    <item>
      <title>July 5th &amp; 6th in Münster: Workshop on Deep Learning with Keras and TensorFlow in R</title>
      <link>/2018/05/deep_learning_keras_tensorflow_18_07/</link>
      <pubDate>Tue, 22 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/deep_learning_keras_tensorflow_18_07/</guid>
      <description>Registration is now open for my 1.5-day workshop on deep learning with Keras and TensorFlow using R.
It will take place on July 5th &amp;amp; 6th in Münster, Germany.

 
You can read about one participant’s experience in my last workshop:
 Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly?</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp; Sandy Huang</title>
      <link>/2018/05/twimlai_adversarial_attacks/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/05/twimlai_adversarial_attacks/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp;amp; Sandy Huang:
Sketchnotes from TWiMLAI talk: Adversarial Attacks Against Reinforcement Learning Agents with Ian Goodfellow &amp;amp; Sandy Huang
 You can listen to the podcast here.
 In this episode, I’m joined by Ian Goodfellow, Staff Research Scientist at Google Brain and Sandy Huang, Phd Student in the EECS department at UC Berkeley, to discuss their work on the paper Adversarial Attacks on Neural Network Policies.</description>
    </item>
    
    <item>
      <title>Comparing dependencies of popular machine learning packages with `pkgnet`</title>
      <link>/2018/04/pkgnet/</link>
      <pubDate>Mon, 30 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/pkgnet/</guid>
      <description>When looking through the CRAN list of packages, I stumbled upon this little gem:
 pkgnet is an R library designed for the analysis of R libraries! The goal of the package is to build a graph representation of a package and its dependencies.
 And I thought it would be fun to play around with it. The little analysis I ended up doing was to compare dependencies of popular machine learning packages.</description>
    </item>
    
    <item>
      <title>Slides from my JAX 2018 talk: Deep Learning - a Primer</title>
      <link>/2018/04/jax2018_slides/</link>
      <pubDate>Fri, 27 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/jax2018_slides/</guid>
      <description>Here I am sharing the slides for a talk that my colleague Uwe Friedrichsen and I gave about Deep Learning - a Primer at the JAX conference on Tuesday, April 24th 2018 in Mainz, Germany.
Slides can be found here: https://www.slideshare.net/ShirinGlander/deep-learning-a-primer-95197733
 Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #121: Reproducibility and the Philosophy of Data with Clare Gollnick</title>
      <link>/2018/04/twimlai121/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/twimlai121/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Reproducibility and the Philosophy of Data with Clare Gollnick:
Sketchnotes from TWiMLAI talk #121: Reproducibility and the Philosophy of Data with Clare Gollnick
 You can listen to the podcast here.
 In this episode, i’m joined by Clare Gollnick, CTO of Terbium Labs, to discuss her thoughts on the “reproducibility crisis” currently haunting the scientific landscape.</description>
    </item>
    
    <item>
      <title>Update: Can we predict flu outcome with Machine Learning in R?</title>
      <link>/2018/04/flu_prediction/</link>
      <pubDate>Sun, 22 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/flu_prediction/</guid>
      <description>Since I migrated my blog from Github Pages to blogdown and Netlify, I wanted to start migrating (most of) my old posts too - and use that opportunity to update them and make sure the code still works.
Here I am updating my very first machine learning post from 27 Nov 2016: Can we predict flu deaths with Machine Learning and R?. Changes are marked as bold comments.
The main changes I made are:</description>
    </item>
    
    <item>
      <title>Look, something shiny: How to use R Shiny to make Münster traffic data accessible. Join MünsteR for our next meetup!</title>
      <link>/2018/04/meetup_june18/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/meetup_june18/</guid>
      <description>In our next MünsteR R-user group meetup on Monday, June 11th, 2018 Thomas Kluth and Thorben Jensen will give a talk titled Look, something shiny: How to use R Shiny to make Münster traffic data accessible. You can RSVP here: http://meetu.ps/e/F7zDN/w54bW/f
 About a year ago, we stumbled upon rich datasets on traffic dynamics of Münster: count data of bikes, cars, and bus passengers of high resolution. Since that day we have been crunching, modeling, and visualizing it.</description>
    </item>
    
    <item>
      <title>HH Data Science Meetup slides: Explaining complex machine learning models with LIME</title>
      <link>/2018/04/hh_datascience_meetup_2018_slides/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/hh_datascience_meetup_2018_slides/</guid>
      <description>On April 12th, 2018 I gave a talk about Explaining complex machine learning models with LIME at the Hamburg Data Science Meetup - so if you’re intersted: the slides can be found here: https://www.slideshare.net/ShirinGlander/hh-data-science-meetup-explaining-complex-machine-learning-models-with-lime-94218890
 Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #124: Systems and Software for Machine Learning at Scale with Jeff Dean</title>
      <link>/2018/04/twimlai124/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/twimlai124/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Systems and Software for Machine Learning at Scale with Jeff Dean:
Sketchnotes from TWiMLAI talk #124: Systems and Software for Machine Learning at Scale with Jeff Dean
 You can listen to the podcast here.
 In this episode I’m joined by Jeff Dean, Google Senior Fellow and head of the company’s deep learning research team Google Brain, who I had a chance to sit down with last week at the Googleplex in Mountain View.</description>
    </item>
    
    <item>
      <title>Meetup slides: Introducing Deep Learning with Keras</title>
      <link>/2018/04/ruhrpy_meetup_2018_slides/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/ruhrpy_meetup_2018_slides/</guid>
      <description>On April 4th, 2018 I gave a talk about Deep Learning with Keras at the Ruhr.Py Meetup in Essen, Germany. The talk was not specific to Python, though - so if you’re intersted: the slides can be found here: https://www.slideshare.net/ShirinGlander/ruhrpy-introducing-deep-learning-with-keras-and-python
  Ruhr.PY - Introducing Deep Learning with Keras and Python  von Shirin Glander  There is also a video recording of my talk, which you can see here: https://youtu.</description>
    </item>
    
    <item>
      <title>Join MünsteR for our next meetup on deep learning with Keras and R</title>
      <link>/2018/03/meetup_april18/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/meetup_april18/</guid>
      <description>In our next MünsteR R-user group meetup on Tuesday, April 17th, 2018 Kai Lichtenberg will talk about deep learning with Keras. You can RSVP here: http://meetu.ps/e/DDY1B/w54bW/f
 Although neural networks have been around for quite a while now, deep learning really just took of a few years ago. It pretty much all started when Alex Krizhevsky and Geoffrey Hinton utterly crushed classic image recognition in the 2012 ImageNet Large Scale Visual Recognition Challenge by implementing a deep neural network with CUDA on graphics cards.</description>
    </item>
    
    <item>
      <title>My upcoming meetup talks about Deep Learning with Keras and explaining complex Machine Learning Models with LIME</title>
      <link>/2018/03/meetup_talk_ruhrpy_april_18/</link>
      <pubDate>Wed, 28 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/meetup_talk_ruhrpy_april_18/</guid>
      <description>I’ll be talking about Deep Learning with Keras in R and Python at the following upcoming meetup:
 Ruhr.Py 2018 on Wednesday, April 4th   Introducing Deep Learning with Keras and Python Keras is a high-level API written in Python for building and prototyping neural networks. It can be used on top of TensorFlow, Theano or CNTK. In this talk we build, train and visualize a Model using Python and Keras - all interactive with Jupyter Notebooks!</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #115: Scaling Machine Learning at Uber with Mike Del Balso</title>
      <link>/2018/03/twimlai115/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/twimlai115/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Scaling Machine Learning at Uber with Mike Del Balso:
Sketchnotes from TWiMLAI talk #115: Scaling Machine Learning at Uber with Mike Del Balso
 You can listen to the podcast here.
 In this episode, I speak with Mike Del Balso, Product Manager for Machine Learning Platforms at Uber. Mike and I sat down last fall at the Georgian Partners Portfolio conference to discuss his presentation “Finding success with machine learning in your company.</description>
    </item>
    
    <item>
      <title>Another Game of Thrones network analysis - this time with tidygraph and ggraph</title>
      <link>/2018/03/got_network/</link>
      <pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/got_network/</guid>
      <description>A while back, I did an analysis of the family network of major characters from the A Song of Ice and Fire books and the Game of Thrones TV show. In that analysis I found out that House Stark (specifically Ned and Sansa) and House Lannister (especially Tyrion) are the most important family connections in Game of Thrones; they also connect many of the story lines and are central parts of the narrative.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic</title>
      <link>/2018/02/twimlai111/</link>
      <pubDate>Mon, 19 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/twimlai111/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Learning “Common Sense” and Physical Concepts with Roland Memisevic:
Sketchnotes from TWiMLAI talk #111: Learning “Common Sense” and Physical Concepts with Roland Memisevic
 You can listen to the podcast here.
 In today’s episode, I’m joined by Roland Memisevic, co-founder, CEO, and chief scientist at Twenty Billion Neurons. Roland joined me at the RE•WORK Deep Learning Summit in Montreal to discuss the work his company is doing to train deep neural networks to understand physical actions.</description>
    </item>
    
    <item>
      <title>April 12th &amp; 13th in Hamburg: Workshop on Deep Learning with Keras and TensorFlow in R</title>
      <link>/2018/02/deep_learning_keras_tensorflow_18_04/</link>
      <pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/deep_learning_keras_tensorflow_18_04/</guid>
      <description>Registration is now open for my 1.5-day workshop on deep learning with Keras and TensorFlow using R.
It will take place on April 12th and 13th in Hamburg, Germany.
You can read about one participant’s experience in my last workshop:
 Big Data – a buzz word you can find everywhere these days, from nerdy blogs to scientific research papers and even in the news. But how does Big Data Analysis work, exactly?</description>
    </item>
    
    <item>
      <title>Announcing my talk about explainability of machine learning models at Minds Mastering Machines conference</title>
      <link>/2018/02/m3_2018/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/m3_2018/</guid>
      <description>On Wednesday, April 25th 2018 I am going to talk about explainability of machine learning models at the Minds Mastering Machines conference in Cologne. The conference will be in German, though.
  ERKLÄRBARKEIT VON MACHINE LEARNING: WIE KÖNNEN WIR VERTRAUEN IN KOMPLEXE MODELLE SCHAFFEN?
  Mit Machine-Learning getroffene Entscheidungen sind inhärent schwierig – wenn nicht gar unmöglich – nachzuvollziehen. Die Komplexität einiger der besten Modelle, wie Neuronale Netzwerke, ist genau das, was sie so erfolgreich macht.</description>
    </item>
    
    <item>
      <title>I talk about machine learning with Daniel Mies (Podcast in German, though)</title>
      <link>/2018/02/herr_mies_wills_wissen/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/herr_mies_wills_wissen/</guid>
      <description>For those of you out there who speak German:
I was interviewed for a tech podcast where I talked about machine learning, neural nets, why I love R and Rstudio and how I became a Data Scientist.
You can download and listen to the podcast here: https://mies.me/2018/01/31/hmww17-machine-learning-mit-dr-shirin-glander/
  In der aktuellen Episode gibt Dr. Shirin Glander (Twitter, Homepage) uns ein paar Einblicke in das Thema Machine Learning. Wir klären zunächst, was Machine Learning ist und welche Möglichkeiten es bietet bevor wir etwas mehr in die Tiefe gehen.</description>
    </item>
    
    <item>
      <title>JAX 2018 talk announcement: Deep Learning - a Primer</title>
      <link>/2018/01/jax2018/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/jax2018/</guid>
      <description>I am happy to announce that on Tuesday, April 24th 2018 Uwe Friedrichsen and I will give a talk about Deep Learning - a Primer at the JAX conference in Mainz, Germany.
 Deep Learning is one of the “hot” topics in the AI area – a lot of hype, a lot of inflated expectation, but also quite some impressive success stories. As some AI experts already predict that Deep Learning will become “Software 2.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley</title>
      <link>/2018/01/twimlai94/</link>
      <pubDate>Sun, 28 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/twimlai94/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley:
Sketchnotes from TWiMLAI talk #94: Neuroevolution: Evolving Novel Neural Network Architectures with Kenneth Stanley
 You can listen to the podcast here.
 Kenneth studied under TWiML Talk #47 guest Risto Miikkulainen at UT Austin, and joined Uber AI Labs after Geometric Intelligence , the company he co-founded with Gary Marcus and others, was acquired in late 2016.</description>
    </item>
    
    <item>
      <title>Join MünsteR for our next meetup on obtaining functional implications of gene expression data with R</title>
      <link>/2018/01/meetup_march18/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/meetup_march18/</guid>
      <description>In our next MünsteR R-user group meetup on March 5th, 2018 Frank Rühle will talk about bioinformatics and how to analyse genome data.
You can RSVP here: http://meetu.ps/e/DDY1B/w54bW/f
 Next-Generation sequencing and array-based technologies provided a plethora of gene expression data in the public genomics databases. But how to get meaningful information and functional implications out of this vast amount of data? Various R-packages have been published by the Bioconductor user community for distinct kinds of analysis strategies.</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #92: Learning State Representations with Yael Niv</title>
      <link>/2018/01/twimlai92/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/twimlai92/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Learning State Representations with Yael Niv: https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/
Sketchnotes from TWiMLAI talk #92: Learning State Representations with Yael Niv
 You can listen to the podcast here.
 In this interview Yael and I explore the relationship between neuroscience and machine learning. In particular, we discusses the importance of state representations in human learning, some of her experimental results in this area, and how a better understanding of representation learning can lead to insights into machine learning problems such as reinforcement and transfer learning.</description>
    </item>
    
    <item>
      <title>How to make your machine learning model available as an API with the plumber package</title>
      <link>/2018/01/plumber/</link>
      <pubDate>Tue, 16 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/plumber/</guid>
      <description>The plumber package for R makes it easy to expose existing R code as a webservice via an API (https://www.rplumber.io/, Trestle Technology, LLC 2017).
You take an existing R script and make it accessible with plumber by simply adding a few lines of comments. If you have worked with Roxygen before, e.g. when building a package, you will already be familiar with the core concepts. If not, here are the most important things to know:</description>
    </item>
    
    <item>
      <title>Sketchnotes from TWiML&amp;AI #91: Philosophy of Intelligence with Matthew Crosby</title>
      <link>/2018/01/twimlai91/</link>
      <pubDate>Sun, 14 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/twimlai91/</guid>
      <description>These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Philosophy of Intelligence with Matthew Crosby:https://twimlai.com/twiml-talk-92-learning-state-representations-yael-niv/
Sketchnotes from TWiMLAI talk #92: Philosophy of Intelligence with Matthew Crosby
You can listen to the podcast here.
This week on the podcast we’re featuring a series of conversations from the NIPs conference in Long Beach, California. I attended a bunch of talks and learned a ton, organized an impromptu roundtable on Building AI Products, and met a bunch of great people, including some former TWiML Talk guests.</description>
    </item>
    
    <item>
      <title>Looking beyond accuracy to improve trust in machine learning</title>
      <link>/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/looking_beyond_accuracy_to_improve_trust_in_ml/</guid>
      <description>I have written another blogpost about Looking beyond accuracy to improve trust in machine learning at my company codecentric&amp;rsquo;s blog:
 Traditional machine learning workflows focus heavily on model training and optimization; the best model is usually chosen via performance measures like accuracy or error and we tend to assume that a model is good enough for deployment if it passes certain thresholds of these performance criteria. Why a model makes the predictions it makes, however, is generally neglected.</description>
    </item>
    
    <item>
      <title>TWiMLAI talk 88 sketchnotes: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru</title>
      <link>/2018/01/twimlai88_sketchnotes/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/twimlai88_sketchnotes/</guid>
      <description>These are my sketchnotes taken from the “This week in Machine Learning &amp;amp; AI” podcast number 88 about Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru:
Sketchnotes from TWiMLAI talk #88: Using Deep Learning and Google Street View to Estimate Demographics with Timnit Gebru
</description>
    </item>
    
    <item>
      <title>Registration now open for workshop on Deep Learning with Keras and TensorFlow using R</title>
      <link>/2017/12/keras_sketchnotes/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/keras_sketchnotes/</guid>
      <description>Recently, I announced my workshop on Deep Learning with Keras and TensorFlow.
The next dates for it are January 18th and 19th in Solingen, Germany.
You can register now by following this link:https://www.codecentric.de/schulung/deep-learning-mit-keras-und-tensorflow
If any non-German-speaking people want to attend, I’m happy to give the course in English!
Contact me if you have further questions.
As a little bonus, I am also sharing my sketch notes from a Podcast I listened to when first getting into Keras:</description>
    </item>
    
    <item>
      <title>MICE (Multiple Imputation by Chained Equations) in R - sketchnotes from MünsteR Meetup</title>
      <link>/2017/11/mice_sketchnotes/</link>
      <pubDate>Tue, 28 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/mice_sketchnotes/</guid>
      <description>Last night, the MünsteR R user-group had another great meetup:
Karin Groothuis-Oudshoorn, Assistant Professor at the University of Twente, presented her R package mice about Multivariate Imputation by Chained Equations.
It was a very interesting talk and here are my sketchnotes that I took during it:
MICE talk sketchnotes
Here is the link to the paper referenced in my notes: https://www.jstatsoft.org/article/view/v045i03
“The mice package implements a method to deal with missing data.</description>
    </item>
    
    <item>
      <title>Workshop on Deep Learning with Keras and TensorFlow in R</title>
      <link>/2017/11/deep_learning_keras_tensorflow/</link>
      <pubDate>Mon, 20 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/deep_learning_keras_tensorflow/</guid>
      <description>You can now book me and my 1-day workshop on deep learning with Keras and TensorFlow using R.
In my workshop, you will learn
the basics of deep learningwhat cross-entropy and loss isabout activation functionshow to optimize weights and biases with backpropagation and gradient descenthow to build (deep) neural networks with Keras and TensorFlowhow to save and load models and model weightshow to visualize models with TensorBoardhow to make predictions on test dataDate and place depend on who and how many people are interested, so please contact me either directly or via the workshop page:https://www.</description>
    </item>
    
    <item>
      <title>How to combine point and boxplots in timeline charts with ggplot2 facets</title>
      <link>/2017/11/combine_point_boxplot_ggplot/</link>
      <pubDate>Sat, 18 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/combine_point_boxplot_ggplot/</guid>
      <description>In a recent project, I was looking to plot data from different variables along the same time axis. The difficulty was, that some of these variables I wanted to have as point plots, while others I wanted as box-plots.
Because I work with the tidyverse, I wanted to produce these plots with ggplot2. Faceting was the obvious first step but it took me quite a while to figure out how to best combine facets with point plots (where I have one value per time point) with and box-plots (where I have multiple values per time point).</description>
    </item>
    
    <item>
      <title>Explore Predictive Maintenance with flexdashboard</title>
      <link>/2017/11/predictive_maintenance_dashboard/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/11/predictive_maintenance_dashboard/</guid>
      <description>I have written the following post about Predictive Maintenance and flexdashboard at my company codecentric’s blog:
Predictive Maintenance is an increasingly popular strategy associated with Industry 4.0; it uses advanced analytics and machine learning to optimize machine costs and output (see Google Trends plot below).A common use-case for Predictive Maintenance is to proactively monitor machines, so as to predict when a check-up is needed to reduce failure and maximize performance.</description>
    </item>
    
    <item>
      <title>Blockchain &amp; distributed ML - my report from the data2day conference</title>
      <link>/2017/09/data2day/</link>
      <pubDate>Thu, 28 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/data2day/</guid>
      <description>Yesterday and today I attended the data2day, a conference about Big Data, Machine Learning and Data Science in Heidelberg, Germany. Topics and workshops covered a range of topics surrounding (big) data analysis and Machine Learning, like Deep Learning, Reinforcement Learning, TensorFlow applications, etc. Distributed systems and scalability were a major part of a lot of the talks as well, reflecting the growing desire to build bigger and more complex models that can’t (or would take too long to) run on a single computer.</description>
    </item>
    
    <item>
      <title>From Biology to Industry. A Blogger’s Journey to Data Science.</title>
      <link>/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./</link>
      <pubDate>Wed, 20 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/from-biology-to-industry.-a-bloggers-journey-to-data-science./</guid>
      <description>Today, I have given a webinar for the Applied Epidemiology Didactic of the University of Wisconsin - Madison titled “From Biology to Industry. A Blogger’s Journey to Data Science.”
I talked about how blogging about R and Data Science helped me become a Data Scientist. I also gave a short introduction to Machine Learning, Big Data and Neural Networks.
My slides can be found here:https://www.slideshare.net/ShirinGlander/from-biology-to-industry-a-bloggers-journey-to-data-science</description>
    </item>
    
    <item>
      <title>Why I use R for Data Science - An Ode to R</title>
      <link>/2017/09/ode_to_r/</link>
      <pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/ode_to_r/</guid>
      <description>Working in Data Science, I often feel like I have to justify using R over Python. And while I do use Python for running scripts in production, I am much more comfortable with the R environment. Basically, whenever I can, I use R for prototyping, testing, visualizing and teaching. But because personal gut-feeling preference isn’t a very good reason to give to (scientifically minded) people, I’ve thought a lot about the pros and cons of using R.</description>
    </item>
    
    <item>
      <title>Moving my blog to blogdown</title>
      <link>/2017/09/moving-my-blog-to-blogdown/</link>
      <pubDate>Tue, 12 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/moving-my-blog-to-blogdown/</guid>
      <description>It’s been a long time coming but I finally moved my blog from Jekyll/Bootstrap on Github pages to blogdown, Hugo and Netlify! Moreover, I also now have my own domain name www.shirin-glander.de. :-)
I followed the blogdown ebook to set up my blog. I chose Thibaud Leprêtre’s tranquilpeak theme. It looks much more polished than my old blog.
My old blog will remain where it is, so that all the links that are out there will still work (and I don’t have to go through the hassle of migrating all my posts to my new site).</description>
    </item>
    
    <item>
      <title>Data Science for Fraud Detection</title>
      <link>/2017/09/data-science-fraud-detection/</link>
      <pubDate>Wed, 06 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/data-science-fraud-detection/</guid>
      <description>I have written the following post about Data Science for Fraud Detection at my company codecentric&amp;rsquo;s blog:
 Fraud can be defined as “the crime of getting money by deceiving people” (Cambridge Dictionary); it is as old as humanity: whenever two parties exchange goods or conduct business there is the potential for one party scamming the other. With an ever increasing use of the internet for shopping, banking, filing insurance claims, etc.</description>
    </item>
    
    <item>
      <title>Migrating from GitHub to GitLab with RStudio (Tutorial)</title>
      <link>/2017/09/migrating-github-gitlab/</link>
      <pubDate>Mon, 04 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/09/migrating-github-gitlab/</guid>
      <description>GitHub vs. GitLab Git is a distributed implementation of version control. Many people have written very eloquently about why it is a good idea to use version control, not only if you collaborate in a team but also if you work on your own; one example is this article from RStudio&amp;rsquo;s Support pages.
In short, its main feature is that version control allows you to keep track of the changes you make to your code.</description>
    </item>
    
    <item>
      <title>Social Network Analysis and Topic Modeling of codecentric’s Twitter friends and followers</title>
      <link>/2017/07/twitter-analysis-codecentric/</link>
      <pubDate>Fri, 28 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/twitter-analysis-codecentric/</guid>
      <description>I have written the following post about Social Network Analysis and Topic Modeling of codecentric&amp;rsquo;s Twitter friends and followers for codecentric&amp;rsquo;s blog:
 Recently, Matthias Radtke has written a very nice blog post on Topic Modeling of the codecentric Blog Articles, where he is giving a comprehensive introduction to Topic Modeling. In this article I am showing a real-world example of how we can use Data Science to gain insights from text data and social network analysis.</description>
    </item>
    
    <item>
      <title>Find all my other posts on my old website!</title>
      <link>/2017/07/find-all-my-other-posts-on-my-old-website/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/find-all-my-other-posts-on-my-old-website/</guid>
      <description>For all my other posts, see my old website: shiring.github.io</description>
    </item>
    
  </channel>
</rss>